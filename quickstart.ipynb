{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cfbf15-7504-4bb1-9a97-fc3f0f8c2314",
   "metadata": {},
   "source": [
    "# **Quickstart de BPETokenizer**\n",
    "\n",
    "¡Bienvenido/a al inicio rápido de `BPETokenizer`!\n",
    "\n",
    "En este notebook, realizaremos un recorrido práctico por las funcionalidades clave de la librería. Aprenderás a:\n",
    "1.  **Instalar** la librería directamente desde GitHub.\n",
    "2.  **Entrenar** un tokenizador BPE desde cero utilizando un corpus de texto (usaremos *El Quijote*).\n",
    "3.  **Guardar** el tokenizador entrenado para usarlo más tarde.\n",
    "4.  **Cargar** y **utilizar** el tokenizador para codificar (tokenizar) y decodificar texto.\n",
    "\n",
    "Este es el lugar perfecto para ver el código en acción. Para una explicación teórica detallada de cómo funciona el algoritmo por dentro, no olvides consultar el `README.md` y el notebook `explicación_bpe.ipynb` en el repositorio.\n",
    "\n",
    "¡Empecemos!\n",
    "\n",
    "## **1. Instalación del Proyecto**\n",
    "\n",
    "El primer paso es instalar la librería `BPETokenizer`. Como este notebook se ejecuta en un entorno limpio (como Google Colab), necesitamos descargarla. El siguiente comando la instala directamente desde su repositorio de GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587066bc-1618-46c2-a132-59cf0a5eaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ErikSarriegui/BPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee7ae2d-997a-4cd5-8e88-c63290786d37",
   "metadata": {},
   "source": [
    "## **2. Entrenamiento de un Nuevo Tokenizador**\n",
    "\n",
    "Ahora que tenemos la librería instalada, vamos a entrenar nuestro propio tokenizador. El proceso consta de los siguientes pasos:\n",
    "\n",
    "1.  **Obtener un corpus de texto**: Para entrenar un tokenizador, necesitamos una buena cantidad de texto. Usaremos la librería `requests` para descargar el texto completo de *Don Quijote de la Mancha*, que utilizaremos como corpus en Español.\n",
    "2.  **Crear un `BPETrainer`**: Esta es la clase encargada de gestionar el proceso de entrenamiento.\n",
    "3.  **Llamar al método `.train()`**: Aquí es donde ocurre la magia. Le pasamos el texto y definimos el tamaño del vocabulario que queremos crear.\n",
    "    *   `vocab_size=270`: Este es el tamaño total del vocabulario. Como partimos de 256 tokens base (un token por cada byte), esto significa que realizaremos `270 - 256 = 14` fusiones. Es un vocabulario muy pequeño, ideal para una demostración rápida.\n",
    "    *   `verbose=True`: Nos permite ver en tiempo real cada una de las 14 fusiones que el algoritmo va aprendiendo. ¡Es perfecto para entender el proceso!\n",
    "4.  **Guardar el tokenizador**: Una vez entrenado, usamos `.save()` para guardar las reglas de fusión y el vocabulario en un directorio. En este caso, lo llamaremos `tokenizer/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e9698-b42d-4e45-b573-8d714400d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BPETokenizer import BPETrainer, BPETokenizer\n",
    "import requests\n",
    "\n",
    "# 1. Obtener el corpus de texto\n",
    "print(\"Descargando el texto de El Quijote...\")\n",
    "\n",
    "text = requests.get(\"https://babel.upm.es/~angel/teaching/pps/quijote.txt\").text\n",
    "print(f\"Texto descargado. Longitud: {len(text)} caracteres.\\n\")\n",
    "\n",
    "# 2. Crear el entrenador\n",
    "trainer = BPETrainer()\n",
    "\n",
    "# 3. Entrenar el tokenizador\n",
    "print(\"Iniciando el entrenamiento del tokenizador...\")\n",
    "trainer.train(\n",
    "    text = text,\n",
    "    vocab_size = 270,  # 256 bytes base + 14 fusiones\n",
    "    verbose = True\n",
    ")\n",
    "print(\"\\n¡Entrenamiento completado!\")\n",
    "\n",
    "# 4. Guardar el tokenizador\n",
    "trainer.save(\"tokenizer\")\n",
    "print(\"Tokenizador guardado en la carpeta 'tokenizer/'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61c594-c920-46b2-b65c-e5a0524f6c47",
   "metadata": {},
   "source": [
    "## **3. Uso del Tokenizador (Inferencia)**\n",
    "\n",
    "¡Nuestro tokenizador ya está entrenado y guardado! Ahora vamos a cargarlo y a usarlo para lo que fue creado: convertir texto en secuencias de números (tokens) y viceversa.\n",
    "\n",
    "1.  **Cargar el tokenizador**: Usamos `BPETokenizer.from_dir()` para cargar la configuración que guardamos en la carpeta `tokenizer/`.\n",
    "2.  **Pre-tokenizar (`.split()`)**: Antes de la tokenización BPE, el texto se divide usando una expresión regular (regex). Este paso separa palabras, puntuación y espacios. El método `.split()` nos permite ver cómo queda el texto después de esta división inicial.\n",
    "3.  **Codificar (`.encode()`)**: Este es el proceso de tokenización principal. Convierte un string de texto en una lista de IDs de tokens numéricos, aplicando las reglas de fusión que aprendió durante el entrenamiento.\n",
    "4.  **Decodificar (`.decode()`)**: Es el proceso inverso. Toma una lista de IDs de tokens y la reconstruye en un string de texto legible por humanos.\n",
    "5.  **Verificar la consistencia**: Finalmente, comprobamos que `decode(encode(texto))` nos devuelve el texto original. Esta es una prueba clave para asegurar que nuestro tokenizador funciona correctamente y no pierde información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b5a8b-9cfe-42ac-92af-17dffb86bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo para probar\n",
    "text = \"Hola mundo\"\n",
    "print(f\"Texto original: '{text}'\\n\")\n",
    "\n",
    "# 1. Cargar el tokenizador\n",
    "tokenizer = BPETokenizer.from_dir(\"tokenizer\")\n",
    "\n",
    "# 2. Ver la pre-tokenización (división por regex)\n",
    "splited_text = tokenizer.split(text)\n",
    "print(f\"Texto pre-tokenizado (split): {splited_text}\")\n",
    "\n",
    "# 3. Codificar el texto a tokens\n",
    "tokenized_text = tokenizer.encode(text)\n",
    "print(f\"Texto codificado (tokens): {tokenized_text}\")\n",
    "\n",
    "# 4. Decodificar los tokens a texto\n",
    "# Usaremos una lista de ejemplo para ver el proceso\n",
    "encoded_text = [72, 111, 108, 97, 32, 109, 117, 110, 100, 111] # 'Hola mundo' en bytes UTF-8\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(f\"Texto decodificado: '{decoded_text}'\")\n",
    "\n",
    "# 5. Verificación de consistencia (ida y vuelta)\n",
    "is_consistent = tokenizer.decode(tokenizer.encode(text)) == text\n",
    "print(f\"\\n¿La decodificación de la codificación devuelve el texto original? -> {is_consistent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
