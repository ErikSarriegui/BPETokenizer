{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59934f2-9e73-4c6c-9ac2-c49a64d98d5c",
   "metadata": {},
   "source": [
    "# **Gu√≠a Interactiva del Algoritmo Byte-Pair Encoding (BPE)**\n",
    "\n",
    "Bienvenido/a a esta gu√≠a interactiva donde desglosaremos, paso a paso, el funcionamiento del algoritmo **Byte-Pair Encoding (BPE)**, la base de los tokenizadores modernos que utilizan los Grandes Modelos de Lenguaje (LLMs).\n",
    "\n",
    "A lo largo de este notebook, cubriremos:\n",
    "1.  **Conceptos Fundamentales:** C√≥mo se representa el texto en un ordenador, desde los strings de Python hasta la codificaci√≥n de bytes en UTF-8.\n",
    "2.  **Implementaci√≥n de BPE desde Cero:** Escribiremos juntos el algoritmo BPE para entender su l√≥gica de fusi√≥n de pares de bytes.\n",
    "3.  **Codificaci√≥n y Decodificaci√≥n:** Veremos c√≥mo usar nuestro tokenizador para convertir texto a tokens y viceversa.\n",
    "4.  **T√©cnicas Avanzadas:** Exploraremos las mejoras que usan los tokenizadores reales, como la pre-tokenizaci√≥n con expresiones regulares (Regex) de GPT-2.\n",
    "5.  **Un Vistazo al Ecosistema:** Analizaremos brevemente otras herramientas est√°ndar de la industria como `tiktoken` de OpenAI y `SentencePiece` de Google.\n",
    "\n",
    "## **1. La Base: De Texto a Bytes**\n",
    "\n",
    "Antes de poder tokenizar, debemos entender c√≥mo un ordenador \"ve\" el texto. Un modelo de lenguaje no opera con letras, sino con n√∫meros.\n",
    "\n",
    "### **1.1. Strings en Python: Secuencias de Puntos de C√≥digo Unicode**\n",
    "\n",
    "Cuando escribes un [`str`](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) en Python, como `\"hola üëã\"`, est√°s creando una secuencia de **puntos de c√≥digo [Unicode](https://es.wikipedia.org/wiki/Unicode)**. Unicode es un est√°ndar universal que asigna un n√∫mero √∫nico a (casi) cualquier car√°cter o s√≠mbolo que puedas imaginar.\n",
    "\n",
    "Podemos ver estos n√∫meros con la funci√≥n `ord()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a7f6c2-770d-455a-bb0c-df7016ec2b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d257be-6e9d-4bf1-939d-7ebd13a4c637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"Hello World\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06b7d06-b0ec-4d12-9d6e-c96809b2feb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128515"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"üòÉ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8e0ab-86e5-499d-947e-9c0c20b28394",
   "metadata": {},
   "source": [
    "### **1.2. De Unicode a Bytes: La Codificaci√≥n UTF-8**\n",
    "\n",
    "Los puntos de c√≥digo Unicode son abstractos. Para almacenar o transmitir texto, necesitamos codificarlos en **bytes**, que son la unidad fundamental de informaci√≥n en un ordenador (un n√∫mero entre 0 y 255).\n",
    "\n",
    "La codificaci√≥n m√°s com√∫n es **UTF-8**, un sistema de longitud variable muy eficiente. Por ejemplo:\n",
    "*   Caracteres [ASCII](https://elcodigoascii.com.ar/) (ingl√©s sin acentos) usan 1 byte.\n",
    "*   Letras con tildes (como '√°') usan 2 bytes.\n",
    "*   Emojis y s√≠mbolos m√°s complejos (‚Ç¨) pueden usar 3 o 4 bytes.\n",
    "\n",
    "En Python, convertimos un `str` a bytes con el m√©todo `.encode(\"utf8\")`. El resultado es una secuencia de enteros entre 0 y 255 (que es el rango de valores que puede tomar un n√∫mero de 1 byte)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6e90e3-3de2-4517-b38d-903561a09d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Hello World\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0b10b-6fb1-4305-a4a7-67772e430eb8",
   "metadata": {},
   "source": [
    "En este caso, esta cadena esta compuesta √∫nicamente por car√°cteres ASCII, por lo que los n√∫meros son los mismos a los vistos arriba en la cadena de `\"Hello World\"`, sin embargo, si vemos el valor que toma el emoji `üòÉ` veremos que es muy diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3503cb9-bf41-443e-a495-83503cc9ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes de üòÉ: b'\\xf0\\x9f\\x98\\x83'\n",
      "\n",
      "Representaci√≥n num√©rica de üòÉ: [240, 159, 152, 131]\n"
     ]
    }
   ],
   "source": [
    "# Ver los bytes\n",
    "raw_bytes = \"üòÉ\".encode(\"utf8\")\n",
    "print(f\"Bytes de üòÉ: {raw_bytes}\")\n",
    "\n",
    "\n",
    "# Representaciones num√©ricas\n",
    "number_bytes = list(\"üòÉ\".encode(\"utf8\"))\n",
    "print(f\"\\nRepresentaci√≥n num√©rica de üòÉ: {number_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16053cc-2f93-4e45-9836-3fc0bc7678d3",
   "metadata": {},
   "source": [
    "Como podemos ver, este caracter es m√°s complejo y requere de 4 bytes para representarse.\n",
    "\n",
    "Esta representaci√≥n en bytes es la materia prima sobre la que opera el algoritmo BPE. Al trabajar con bytes, nos aseguramos de que **nunca habr√° un car√°cter \"desconocido\"**. Cualquier texto, en cualquier idioma, puede descomponerse en una secuencia de bytes.\n",
    "\n",
    "## **2. El Algoritmo BPE: Paso a Paso**\n",
    "\n",
    "El algoritmo BPE es un m√©todo de compresi√≥n de datos que busca iterativamente el par de bytes m√°s frecuente en un texto y lo reemplaza con un nuevo byte (o, en nuestro caso, un nuevo token con un ID mayor que 255).\n",
    "\n",
    "Vamos a implementarlo desde cero con un fragmento de *El Quijote*.\n",
    "\n",
    "### **2.1. Nuestro Corpus de Entrenamiento**\n",
    "\n",
    "Primero, convertimos nuestro texto a una secuencia de bytes (representados como enteros)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22d8a14-b3e4-48ed-ae26-0595ac661ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El corpus tiene 130 caracteres\n",
      "El corpus tiene 131 bytes\n"
     ]
    }
   ],
   "source": [
    "text = \"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que viv√≠a un hidalgo de los de lanza en astillero\"\n",
    "print(f\"El corpus tiene {len(text)} caracteres\")\n",
    "\n",
    "text_bytes = text.encode(\"utf8\")\n",
    "print(f\"El corpus tiene {len(text_bytes)} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0875f09-7c21-439b-b4f4-c5df1b53cdfc",
   "metadata": {},
   "source": [
    "Hmmm... ¬øC√≥mo puede ser que tengamos m√°s bytes que caracteres?\n",
    "\n",
    "Si has prestado anteci√≥n, te habr√°s dado cuenta que el texto contiene una tilde (en `\"viv√≠a\"`) cuya representaci√≥n no equivale solo a un byte.\n",
    "\n",
    "### **2.2. Encontrar el Par M√°s Frecuente**\n",
    "\n",
    "Ahora, necesitamos una funci√≥n que recorra la secuencia de tokens y cuente la frecuencia de cada par adyacente, para esto utilizaremos la funci√≥n `get_stats()`, que nos proporciona una lista, ordenada de mayor a menor frecuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9012d87-8bc5-463c-8229-61cf825fd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, (111, 32)), (6, (101, 32)), (4, (110, 32)), (4, (100, 101)), (4, (97, 32)), (4, (32, 108)), (4, (32, 100)), (3, (110, 111)), (3, (32, 110)), (2, (117, 110)), (2, (116, 105)), (2, (114, 111)), (2, (113, 117)), (2, (108, 97)), (2, (105, 101)), (2, (104, 97)), (2, (101, 114)), (2, (100, 97)), (2, (99, 104)), (2, (97, 114)), (2, (97, 110)), (2, (44, 32)), (2, (32, 117)), (2, (32, 113)), (2, (32, 104)), (2, (32, 97)), (1, (195, 173)), (1, (173, 97)), (1, (122, 97)), (1, (121, 111)), (1, (118, 195)), (1, (118, 105)), (1, (117, 121)), (1, (117, 105)), (1, (117, 103)), (1, (117, 101)), (1, (117, 99)), (1, (115, 116)), (1, (115, 32)), (1, (114, 109)), (1, (114, 101)), (1, (114, 100)), (1, (114, 32)), (1, (112, 111)), (1, (111, 115)), (1, (111, 114)), (1, (111, 109)), (1, (110, 122)), (1, (110, 99)), (1, (109, 117)), (1, (109, 112)), (1, (109, 101)), (1, (109, 98)), (1, (108, 117)), (1, (108, 111)), (1, (108, 108)), (1, (108, 103)), (1, (108, 101)), (1, (105, 118)), (1, (105, 108)), (1, (105, 100)), (1, (104, 111)), (1, (104, 105)), (1, (103, 111)), (1, (103, 97)), (1, (101, 110)), (1, (101, 109)), (1, (101, 44)), (1, (99, 117)), (1, (99, 111)), (1, (98, 114)), (1, (97, 115)), (1, (97, 108)), (1, (97, 99)), (1, (97, 44)), (1, (77, 97)), (1, (69, 110)), (1, (32, 118)), (1, (32, 116)), (1, (32, 109)), (1, (32, 101)), (1, (32, 99)), (1, (32, 77))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(text_bytes)\n",
    "print(sorted(((v, k) for k, v in stats.items()), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faaf1be-3eaa-449a-b65e-5ed0e2e0c42a",
   "metadata": {},
   "source": [
    "En nuestro caso, el par que m√°s veces aparece es el `(111, 32)`. Que podemos ver que corresponde a..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1866e5-c5a7-4240-aaf1-a1e1e2e6bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El par m√°s frecuente es: ' o'\n"
     ]
    }
   ],
   "source": [
    "pair = bytes([32]).decode(\"utf8\") + bytes([111]).decode(\"utf8\")\n",
    "print(f\"El par m√°s frecuente es: '{pair}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba236b-bfd5-4d6c-865e-228b26fbaa96",
   "metadata": {},
   "source": [
    "¬°Es la letra \"o\" seguida de un espacio! Esto tiene sentido, ya que muchas palabras en espa√±ol terminan en \"o\".\n",
    "\n",
    "### **2.3. Fusionar el Par y Crear un Nuevo Token**\n",
    "\n",
    "Ahora, reemplazamos todas las ocurrencias del par `(111, 32)` con un nuevo token. Nuestro vocabulario inicial son los 256 bytes (IDs de 0 a 255), as√≠ que el primer token nuevo tendr√° el ID `256`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a577bf93-afe7-4a59-94df-48dca16c7c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "top_pair = (111, 32)\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = list()\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "text_bytes = merge(text_bytes, top_pair, 256)\n",
    "print(len(text_bytes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7bf98-252e-4eae-ac33-33457c919312",
   "metadata": {},
   "source": [
    "La longitud de nuestra secuencia se ha reducido de 131 a 124. Esto es `131 - 7 = 124`, ¬°exactamente las 7 ocurrencias del par `('o', ' ')` que fusionamos!\n",
    "\n",
    "### **2.4. El Proceso Iterativo**\n",
    "\n",
    "Repetimos este proceso (buscar el par m√°s frecuente, fusionarlo y reemplazarlo) un n√∫mero determinado de veces. Este n√∫mero lo define el `vocab_size` que deseemos. Si queremos un vocabulario de 276 tokens, realizaremos `276 - 256 = 20` fusiones.\n",
    "\n",
    "Vamos a automatizarlo en un bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d4951d-18c0-424f-8bad-76c891f48b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusionando (101, 32) -> 256\n",
      "Fusionando (110, 32) -> 257\n",
      "Fusionando (100, 256) -> 258\n",
      "Fusionando (97, 32) -> 259\n",
      "Fusionando (32, 258) -> 260\n",
      "Fusionando (117, 257) -> 261\n",
      "Fusionando (97, 114) -> 262\n",
      "Fusionando (260, 108) -> 263\n",
      "Fusionando (97, 110) -> 264\n",
      "Fusionando (99, 104) -> 265\n",
      "Fusionando (256, 110) -> 266\n",
      "Fusionando (256, 113) -> 267\n",
      "Fusionando (267, 117) -> 268\n",
      "Fusionando (105, 101) -> 269\n",
      "Fusionando (69, 257) -> 270\n",
      "Fusionando (270, 261) -> 271\n",
      "Fusionando (271, 108) -> 272\n",
      "Fusionando (272, 117) -> 273\n",
      "Fusionando (273, 103) -> 274\n",
      "Fusionando (274, 262) -> 275\n"
     ]
    }
   ],
   "source": [
    "# --- COMENTARIOS SUGERIDOS PARA EL C√ìDIGO ---\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "# Copiamos la lista de tokens para no modificar la original\n",
    "ids = list(text_bytes)\n",
    "\n",
    "# El diccionario 'merges' almacenar√° las reglas de fusi√≥n aprendidas en orden\n",
    "# (par) -> nuevo_id\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Fusionando {pair} -> {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33824b4-8902-440d-8367-4b285b07aa17",
   "metadata": {},
   "source": [
    "### **2.5. Resultados de la Compresi√≥n**\n",
    "\n",
    "Despu√©s de 20 fusiones, veamos cu√°nto hemos comprimido nuestra secuencia de texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b274bd3e-5cfb-444c-85ae-5b556beb735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length:  124\n",
      "ids length:  79\n",
      "compression ratio: 1.57X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length: \", len(text_bytes))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(f\"compression ratio: {len(text_bytes) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e0612-fe4e-4a90-af06-630df6e5e11c",
   "metadata": {},
   "source": [
    "## **3. Codificaci√≥n y Decodificaci√≥n**\n",
    "\n",
    "Un tokenizador debe ser capaz de hacer dos cosas: **codificar** (texto -> tokens) y **decodificar** (tokens -> texto).\n",
    "\n",
    "### **3.1. Decodificaci√≥n**\n",
    "\n",
    "Para decodificar, necesitamos un \"vocabulario\" que mapee cada ID de token (tanto los 256 originales como los nuevos que creamos) a su secuencia de bytes correspondiente. Lo construimos a partir de nuestras `merges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3271f0dd-86b1-4387-92cc-15f2381e85df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el vocabulario inicial con los 256 bytes base.\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "\n",
    "# A√±adimos los nuevos tokens fusionados. Cada nuevo token es la concatenaci√≥n de los bytes de los dos tokens que lo formaron.\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # Recuperamos la secuencia de bytes completa concatenando los bytes de cada token.\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    \n",
    "    # Decodificamos la secuencia de bytes a un string, reemplazando errores.\n",
    "    # 'errors=\"replace\"' es una salvaguarda importante: si el LLM genera una secuencia de bytes inv√°lida en UTF-8, no se romper√°, sino que mostrar√° un car√°cter de reemplazo 'ÔøΩ'.\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440325c0-42e5-47f3-b43b-f95800afc0e1",
   "metadata": {},
   "source": [
    "### **3.2. Codificaci√≥n**\n",
    "\n",
    "La codificaci√≥n de un texto nuevo es el proceso de aplicar las fusiones aprendidas, **en el mismo orden en que se aprendieron**. Esto es crucial, ya que algunas fusiones dependen de otras anteriores.\n",
    "\n",
    "La estrategia es:\n",
    "1. Convertir el texto a su secuencia de bytes inicial.\n",
    "2. Buscar en la secuencia todos los pares de tokens posibles.\n",
    "3. De todos los pares que existen en nuestras `merges` aprendidas, aplicar el que se aprendi√≥ primero (el que tiene el ID m√°s bajo).\n",
    "4. Repetir hasta que no se puedan hacer m√°s fusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089b082b-3799-4b95-a812-50776543213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)\n",
    "        # En lugar de buscar el par m√°s frecuente, buscamos el par que aparece en nuestra lista de 'merges' con el √≠ndice m√°s bajo.\n",
    "        # merges.get(p, float(\"inf\")) devuelve el √≠ndice de fusi√≥n para un par 'p', o infinito si el par nunca fue fusionado.\n",
    "        # Al buscar el 'min', encontramos la fusi√≥n que se aprendi√≥ primero.\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        # Si el par con el √≠ndice m√°s bajo es 'inf', significa que no hay m√°s pares fusionables en el texto.\n",
    "        if pair not in merges:\n",
    "            break # No hay nada m√°s que fusionar\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692a844c-36be-44bb-8ca3-a7e197352267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øLa decodificaci√≥n de la codificaci√≥n devuelve el texto original? -> True\n"
     ]
    }
   ],
   "source": [
    "text = \"Como est√°s?\"\n",
    "\n",
    "encoded = encode(text)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "is_consistent = decode(encode(text)) == text\n",
    "print(f\"¬øLa decodificaci√≥n de la codificaci√≥n devuelve el texto original? -> {is_consistent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c8b951-1c0c-4c4b-83f8-e74f06d0e9fb",
   "metadata": {},
   "source": [
    "# **4. Mejorando Nuestro Tokenizador: El Enfoque de GPT**\n",
    "\n",
    "Nuestro tokenizador funciona, pero los tokenizadores de producci√≥n como los de la familia GPT introducen algunas mejoras clave.\n",
    "\n",
    "### **4.1. Pre-tokenizaci√≥n con Expresiones Regulares (Regex)**\n",
    "\n",
    "Si aplicamos BPE directamente sobre un texto, podr√≠amos fusionar el final de una palabra con el principio de la siguiente:\n",
    "\n",
    "Imagina que en nuestro texto de entrenamiento aparecen con mucha frecuencia palabras terminadas en \"a\" seguidas de un punto, como en \"problema.\", \"casa.\" o \"idea.\".\n",
    "\n",
    "El par de bytes para a. (la letra 'a' y el punto) se volver√≠a muy frecuente. El algoritmo BPE, sin ninguna gu√≠a, crear√≠a felizmente un nuevo token para a..\n",
    "\n",
    "Ahora, cuando el tokenizador vea la palabra \"Hola.\", la podr√≠a dividir en:\n",
    "\n",
    "`[\"Hol\", \"a.\"]` en lugar de la forma mucho m√°s l√≥gica `[\"Hola\", \".\"]`.\n",
    "\n",
    "**¬øPor qu√© es esto un problema?**\n",
    "\n",
    "1. **P√©rdida de Informaci√≥n Gramatical**: El token . por s√≠ solo tiene un significado muy fuerte: \"fin de la oraci√≥n\". Al fusionarlo con \"a\", diluimos esa se√±al. El modelo tendr√≠a que aprender por separado el significado de a., o., s., etc., en lugar de aprender el concepto universal del punto final.\n",
    "2. **Ineficiencia del Vocabulario**: Se generan tokens redundantes que mezclan conceptos. Es mucho m√°s eficiente tener un token para \"Hola\" y otro para . que pueden combinarse de m√∫ltiples maneras.\n",
    "\n",
    "Para evitarlo, GPT-2 y modelos posteriores usan una **pre-tokenizaci√≥n** basada en expresiones regulares. El texto se divide primero en \"trozos\" que suelen corresponder a palabras, puntuaci√≥n o espacios. El algoritmo BPE se ejecuta de forma independiente dentro de cada uno de estos trozos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e46b824-b396-4986-9786-e06e9eba1bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', ' you', ' know', ' where', ' my', ' 1', 'st', ' dog', ' is', '?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", re.IGNORECASE)\n",
    "\n",
    "text = \"Do you know where my 1st dog is?\"\n",
    "print(re.findall(gpt2pat, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a4f008-88c3-4ce7-a5c0-cf6af20485ab",
   "metadata": {},
   "source": [
    "Esto asegura que las fusiones respeten los l√≠mites de las palabras, generando tokens mucho m√°s intuitivos. Pero, ¬øc√≥mo cambia exactamente el algoritmo BPE que implementamos antes?\n",
    "\n",
    "La diferencia fundamental es el **√°mbito de aplicaci√≥n**. El algoritmo BPE ya no se ejecuta sobre una √∫nica y larga secuencia de bytes de todo el corpus. En su lugar, el proceso se modifica de la siguiente manera:\n",
    "\n",
    "1.  **Divisi√≥n Primero**: El texto completo se divide primero en una lista de \"trozos\" utilizando la expresi√≥n regular.\n",
    "2.  **BPE Dentro de Cada Trozo**: El algoritmo BPE se ejecuta de forma **independiente y aislada dentro de cada uno de estos trozos**.\n",
    "\n",
    "Esto significa que una fusi√≥n de bytes **nunca puede ocurrir a trav√©s de los l√≠mites de dos trozos diferentes**.\n",
    "\n",
    "#### **Ejemplo Pr√°ctico: \"coche azul\"**\n",
    "\n",
    "Veamos el impacto con un ejemplo sencillo:\n",
    "\n",
    "*   **Sin Pre-tokenizaci√≥n (M√©todo Antiguo):**\n",
    "    *   El texto es una √∫nica secuencia de bytes: `b'coche azul'`.\n",
    "    *   El algoritmo BPE podr√≠a analizar el par `(e,  )` (los bytes de la 'e' de \"coche\" y el espacio siguiente) y, si es muy frecuente en el corpus, fusionarlo. Esto es sem√°nticamente indeseable.\n",
    "\n",
    "*   **Con Pre-tokenizaci√≥n (M√©todo GPT-2):**\n",
    "    *   El regex divide el texto en una lista de trozos: `['coche', ' ', 'azul']`.\n",
    "    *   El algoritmo BPE se ejecuta tres veces por separado:\n",
    "        1.  **Sobre `b'coche'`: ** Buscar√° y fusionar√° los pares de bytes m√°s comunes *solo dentro de esta palabra*.\n",
    "        2.  **Sobre `b' '`: ** No hay pares que fusionar.\n",
    "        3.  **Sobre `b'azul'`: ** Buscar√° y fusionar√° los pares de bytes m√°s comunes *solo dentro de esta palabra*.\n",
    "\n",
    "### **4.2. Tokens Especiales**\n",
    "\n",
    "A menudo necesitamos tokens que no se derivan del texto, sino que act√∫an como instrucciones para el modelo. Ejemplos:\n",
    "- `<|endoftext|>`: Indica el final de un documento.\n",
    "- `<|im_start|>`: Marca el inicio de un mensaje en un chat.\n",
    "\n",
    "Estos tokens se a√±aden al vocabulario **despu√©s** del entrenamiento de BPE con IDs reservados.\n",
    "\n",
    "### **4.3. Un Vistazo a `tiktoken`**\n",
    "\n",
    "`tiktoken` es la librer√≠a de tokenizaci√≥n de OpenAI. Es solo para inferencia (no se puede entrenar con ella), pero nos permite ver las diferencias entre tokenizadores, como el manejo de los espacios en GPT-2 vs GPT-4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa7c254f-1ced-45b9-8ed2-dc829db5af1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: '    Hello World'\n",
      "\n",
      "--- Tokenizador de GPT-2 (gpt2) ---\n",
      "Tokens: [220, 220, 220, 18435, 2159]\n",
      "N√∫mero de tokens: 5\n",
      "\n",
      "--- Tokenizador de GPT-4 (cl100k_base) ---\n",
      "Tokens: [262, 22691, 4435]\n",
      "N√∫mero de tokens: 3\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Texto de ejemplo con m√∫ltiples espacios al inicio\n",
    "text_to_encode = \"    Hello World\"\n",
    "print(f\"Texto original: '{text_to_encode}'\\n\")\n",
    "\n",
    "# --- Tokenizador de GPT-2 ---\n",
    "# Este tokenizador tiende a tratar cada espacio como un token separado.\n",
    "print(\"--- Tokenizador de GPT-2 (gpt2) ---\")\n",
    "enc_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens_gpt2 = enc_gpt2.encode(text_to_encode)\n",
    "print(f\"Tokens: {tokens_gpt2}\")\n",
    "print(f\"N√∫mero de tokens: {len(tokens_gpt2)}\")\n",
    "\n",
    "\n",
    "# --- Tokenizador de GPT-4 ---\n",
    "# 'cl100k_base' es el tokenizador utilizado por modelos como GPT-3.5-turbo y GPT-4.\n",
    "# Es m√°s eficiente y ha aprendido a fusionar secuencias de espacios.\n",
    "print(\"\\n--- Tokenizador de GPT-4 (cl100k_base) ---\")\n",
    "enc_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens_gpt4 = enc_gpt4.encode(text_to_encode)\n",
    "print(f\"Tokens: {tokens_gpt4}\")\n",
    "print(f\"N√∫mero de tokens: {len(tokens_gpt4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd337a3e-3460-44b1-9256-8adcbe9741f7",
   "metadata": {},
   "source": [
    "Como vemos, GPT-4 aprendi√≥ a fusionar secuencias de espacios, mientras que GPT-2 las trata como tokens individuales. Estas decisiones de dise√±o afectan a la eficiencia y el comportamiento del modelo.\n",
    "\n",
    "## **5. Alternativas: SentencePiece de Google**\n",
    "\n",
    "`SentencePiece` es otra librer√≠a de tokenizaci√≥n muy popular, utilizada en modelos como Llama y Mistral. Permite tanto el entrenamiento como la inferencia.\n",
    "\n",
    "Su principal diferencia filos√≥fica con `tiktoken` es:\n",
    "*   **tiktoken (GPT-style):** Trabaja directamente sobre la secuencia de **bytes UTF-8**. Es universal.\n",
    "*   **SentencePiece:** Trabaja principalmente con **puntos de c√≥digo Unicode**. No convierte a bytes a menos que encuentre un car√°cter desconocido, en cuyo caso utiliza un mecanismo de `byte_fallback`.\n",
    "\n",
    "Vamos a entrenar un tokenizador `SentencePiece` b√°sico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa1287-a02b-48c3-befa-3b2adfbf2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import requests\n",
    "\n",
    "URL = \"https://github.com/ErikSarriegui/BPETokenizer/blob/main/text/toy_text.txt\"\n",
    "with open(\"text/toy_text.txt\", \"w\") as file:\n",
    "    file.write(requests.get(URL).text)\n",
    "\n",
    "options = dict(\n",
    "    # --- Input/Output ---\n",
    "    input=\"text/toy_text.txt\",                   # Fichero de texto para entrenar\n",
    "    model_prefix=\"tok400\",                  # Prefijo para los ficheros de salida (.model, .vocab)\n",
    "    \n",
    "    # --- Algoritmo ---\n",
    "    model_type=\"bpe\",                       # Usamos el algoritmo BPE\n",
    "    vocab_size=400,                         # Tama√±o del vocabulario deseado\n",
    "    byte_fallback=True,                     # Si no conoce un car√°cter, lo descompone en sus bytes UTF-8\n",
    "    \n",
    "    # --- Normalizaci√≥n y Pre-tokenizaci√≥n ---\n",
    "    normalization_rule_name=\"identity\",     # 'identity' = no hacer normalizaci√≥n (ej: a NFC). Se recomienda no tocar el texto.\n",
    "    remove_extra_whitespaces=False,         # No eliminar espacios extra\n",
    "    split_digits=True,                      # Tratar cada d√≠gito como un token separado al inicio\n",
    "    add_dummy_prefix=True,                  # A√±ade un espacio al inicio de la frase para que \" a\" se tokenice igual que el \" a\" de \"una casa\".\n",
    "    \n",
    "    # --- Tokens Especiales ---\n",
    "    unk_id=0,                               # ID para token desconocido (si byte_fallback=False)\n",
    "    bos_id=1,                               # ID para \"Beginning of Sentence\"\n",
    "    eos_id=2,                               # ID para \"End of Sentence\"\n",
    "    pad_id=-1,                              # ID para \"Padding\" (ignorado, no se a√±ade al vocabulario)\n",
    ")\n",
    "\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bf3deb4-4f3d-4578-b7e5-b55da1166a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto a codificar: 'm√°s'\n",
      "IDs resultantes: [323, 350, 198, 164, 331]\n",
      "\n",
      "Piezas decodificadas: ['‚ñÅ', 'm', '<0xC3>', '<0xA1>', 's']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tok400.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx ] for idx in range(sp.get_piece_size())]\n",
    "\n",
    "texto_de_prueba = \"m√°s\"\n",
    "print(f\"Texto a codificar: '{texto_de_prueba}'\")\n",
    "\n",
    "# Codificamos el texto en una secuencia de IDs num√©ricos.\n",
    "ids = sp.encode(texto_de_prueba)\n",
    "print(f\"IDs resultantes: {ids}\\n\")\n",
    "\n",
    "# Ahora, decodificamos los IDs de vuelta a sus \"piezas\" para inspeccionar qu√© hizo el tokenizador.\n",
    "# Esta es la parte m√°s instructiva.\n",
    "piezas_decodificadas = [sp.id_to_piece(id) for id in ids]\n",
    "print(f\"Piezas decodificadas: {piezas_decodificadas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8492ab-b0ee-49d5-8c63-5ce2fb3bf9ba",
   "metadata": {},
   "source": [
    "Observa c√≥mo \"m√°s\" se tokeniza. Como nuestro corpus de entrenamiento no conten√≠a la letra \"√°\", `SentencePiece` no tiene un token para ella. Gracias a `byte_fallback=True`, la descompone en sus dos bytes UTF-8. Sin esta opci√≥n, la habr√≠a reemplazado por el token `<unk>`.\n",
    "\n",
    "## **6. Conclusiones Finales**\n",
    "\n",
    "Hemos recorrido un largo camino, desde un simple `ord('h')` hasta entrenar tokenizadores con librer√≠as est√°ndar.\n",
    "\n",
    "#### **Sobre el Tama√±o del Vocabulario (`vocab_size`)**\n",
    "Elegir el `vocab_size` es un compromiso:\n",
    "*   **Vocabulario peque√±o:**\n",
    "    *   **Pros:** Menor coste computacional (capa de embeddings y de salida m√°s peque√±as).\n",
    "    *   **Contras:** Las secuencias de tokens son m√°s largas, lo que aumenta el coste en el Transformer. Menos informaci√≥n por token.\n",
    "*   **Vocabulario grande:**\n",
    "    *   **Pros:** Secuencias de tokens m√°s cortas (m√°s eficiente para el Transformer). Tokens sem√°nticamente m√°s ricos.\n",
    "    *   **Contras:** M√°s memoria y coste computacional en las capas de entrada/salida. Riesgo de que algunos tokens aparezcan tan poco que el modelo no aprenda bien su significado.\n",
    "\n",
    "Los modelos actuales suelen usar vocabularios de decenas de miles (GPT-2: ~50k, Llama 3: 128k).\n",
    "\n",
    "#### **Sobre la Multimodalidad**\n",
    "¬øC√≥mo manejan los modelos texto e im√°genes? La clave est√° en la tokenizaci√≥n. No se cambia la arquitectura del Transformer, sino que se dise√±a un \"tokenizador de im√°genes\" (normalmente una Red Neuronal Convolucional o un ViT) que convierte una imagen en una secuencia de vectores (tokens). Para el Transformer, son solo una secuencia de tokens m√°s, indistinguibles de los que provienen del texto.\n",
    "\n",
    "¬°Gracias por seguir esta gu√≠a! Ahora tienes una base s√≥lida para entender c√≥mo los LLMs procesan el lenguaje."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
