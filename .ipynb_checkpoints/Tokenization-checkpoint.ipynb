{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3231c4-7f8c-4fc8-88b8-e1387d2b9712",
   "metadata": {},
   "source": [
    "# **Tokenization**\n",
    "La tokenización es el proceso mediante el cual los inputs de texto de un modelo de lenguaje se transforman a una representación numérica.\n",
    "\n",
    "## **Strings en Python**\n",
    "Los [strings en Python](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) son secuencias innumables de valores codificados en [unicode](https://es.wikipedia.org/wiki/Unicode). Se puede acceder utilizando `ord()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53548c0a-93dc-48b6-8918-521647afd350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282a887c-8741-4750-8d1a-0d88cbf0b970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"Hello World\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554cc6f9-351f-4f2d-9013-6aa4eb606fb4",
   "metadata": {},
   "source": [
    "Unicode tiene tres manera de codificar estos valores en unicode y traducirlos a bytes: UTF-8,UTF-16, UTF-32... Lo que hacen es traducir todos los códigos de Unicode a valores de 4 bytes. UTF-8 es el que más se utiliza porque tiene retrocompatibilidad con ASCII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e736c65-e098-4c22-80cf-db9dcabfd8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Hello World\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f64c25-1d63-4464-91bd-2dd3b0a8ba5c",
   "metadata": {},
   "source": [
    "El problema de utilizar esto es que nuestras secuencias de texto serán largísimas. Para resolver esto acudimos al algoritmo [Byte Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding) para comprimir las secuencias de bytes. Byte Pair Encoding lo que hace es ir iterando sobre la cadena de texto encontrando los pares que caracteres que más se repiten y sustituyendolos por un token nuevo.\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "Se sustituyen las `aa` por `Z`:\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Se sustituyen las `ab` por `Y`:\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Se sustituyen las `ZY` por `X`:\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Aplicación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f4d00e-49ae-4c32-8f27-6c9e3fd232e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor.\n",
      "Número de caracteres: 177\n",
      "\n",
      "---\n",
      "[69, 110, 32, 117, 110, 32, 108, 117, 103, 97, 114, 32, 100, 101, 32, 108, 97, 32, 77, 97, 110, 99, 104, 97, 44, 32, 100, 101, 32, 99, 117, 121, 111, 32, 110, 111, 109, 98, 114, 101, 32, 110, 111, 32, 113, 117, 105, 101, 114, 111, 32, 97, 99, 111, 114, 100, 97, 114, 109, 101, 44, 32, 110, 111, 32, 104, 97, 32, 109, 117, 99, 104, 111, 32, 116, 105, 101, 109, 112, 111, 32, 113, 117, 101, 32, 118, 105, 118, 195, 173, 97, 32, 117, 110, 32, 104, 105, 100, 97, 108, 103, 111, 32, 100, 101, 32, 108, 111, 115, 32, 100, 101, 32, 108, 97, 110, 122, 97, 32, 101, 110, 32, 97, 115, 116, 105, 108, 108, 101, 114, 111, 44, 32, 97, 100, 97, 114, 103, 97, 32, 97, 110, 116, 105, 103, 117, 97, 44, 32, 114, 111, 99, 195, 173, 110, 32, 102, 108, 97, 99, 111, 32, 121, 32, 103, 97, 108, 103, 111, 32, 99, 111, 114, 114, 101, 100, 111, 114, 46]\n",
      "Número de bytes 179\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor.\"\"\"\n",
    "\n",
    "tokens = text.encode(\"utf8\")\n",
    "tokens = list(map(int, tokens))\n",
    "\n",
    "print(\"---\")\n",
    "print(text)\n",
    "print(f\"Número de caracteres: {len(text)}\")\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(tokens)\n",
    "print(f\"Número de bytes {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c126638-638e-48ff-8207-d0b5bcdfe80a",
   "metadata": {},
   "source": [
    "Hay más bytes que caracteres ya que los bytes a veces se unen para combertirse en caracteres (cómo las tíldes, por ejemplo). Ahora queremos iterar y encontrar el número de bytes que se repite más a menudo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43e63db-8b69-4984-bd48-489dc863e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(9, (111, 32)), (6, (101, 32)), (5, (110, 32)), (5, (97, 32)), (4, (100, 101)), (4, (44, 32)), (4, (32, 108)), (4, (32, 100)), (4, (32, 97)), (3, (116, 105)), (3, (114, 111)), (3, (111, 114)), (3, (110, 111)), (3, (108, 97)), (3, (103, 97)), (3, (100, 97)), (3, (99, 111)), (3, (97, 114)), (3, (97, 110)), (3, (32, 110)), (2, (195, 173)), (2, (117, 110)), (2, (114, 101)), (2, (113, 117)), (2, (108, 103)), (2, (105, 101)), (2, (104, 97)), (2, (103, 111)), (2, (101, 114)), (2, (99, 104)), (2, (97, 108)), (2, (97, 99)), (2, (97, 44)), (2, (32, 117)), (2, (32, 113)), (2, (32, 104)), (2, (32, 99)), (1, (173, 110)), (1, (173, 97)), (1, (122, 97)), (1, (121, 111)), (1, (121, 32)), (1, (118, 195)), (1, (118, 105)), (1, (117, 121)), (1, (117, 105)), (1, (117, 103)), (1, (117, 101)), (1, (117, 99)), (1, (117, 97)), (1, (115, 116)), (1, (115, 32)), (1, (114, 114)), (1, (114, 109)), (1, (114, 103)), (1, (114, 100)), (1, (114, 46)), (1, (114, 32)), (1, (112, 111)), (1, (111, 115)), (1, (111, 109)), (1, (111, 99)), (1, (111, 44)), (1, (110, 122)), (1, (110, 116)), (1, (110, 99)), (1, (109, 117)), (1, (109, 112)), (1, (109, 101)), (1, (109, 98)), (1, (108, 117)), (1, (108, 111)), (1, (108, 108)), (1, (108, 101)), (1, (105, 118)), (1, (105, 108)), (1, (105, 103)), (1, (105, 100)), (1, (104, 111)), (1, (104, 105)), (1, (103, 117)), (1, (102, 108)), (1, (101, 110)), (1, (101, 109)), (1, (101, 100)), (1, (101, 44)), (1, (100, 111)), (1, (99, 195)), (1, (99, 117)), (1, (98, 114)), (1, (97, 115)), (1, (97, 100)), (1, (77, 97)), (1, (69, 110)), (1, (32, 121)), (1, (32, 118)), (1, (32, 116)), (1, (32, 114)), (1, (32, 109)), (1, (32, 103)), (1, (32, 102)), (1, (32, 101)), (1, (32, 77))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(sorted(((v, k) for k, v in stats.items()), reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a11e2-8f79-4ed5-b7a4-6592f600fa50",
   "metadata": {},
   "source": [
    "Aquí tenemos una lista con las veces que se repiten cada uno de los pares, en nuestro caso el que más se repite es el par compuesto de `111`, `32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e3e90c-c65a-472d-a24e-d225c264f220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111, 32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key = stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "407c0756-2c6c-4930-875b-7df72d47cf49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('o', ' ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(111), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64649eb1-c43d-4fd3-ad8a-8a367ca2066f",
   "metadata": {},
   "source": [
    "Parece que tenemos muchos `o ` es decir, la letra o seguido de un espacio. Ahora lo que debemos hacer es asignarle un token, que será el 256, ya que hasta el 255 ya está ocupado. Esto porque la codificación UTF-8 va del 0 al 255 (8 bits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e07b9f-8551-4f15-93ea-99ac0bc2da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = list()\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195415b-84c8-4027-8032-bd7503c4573e",
   "metadata": {},
   "source": [
    "Ha salido 170 de longitud, que coincide con 179 que había antes - 9 repetidos del par mayor. Ahora ya podemos hacer un bucle que haga el trabajo por nosotros. Sin embargo, surge la pregunta de... *¿Hasta cuando lo hacemos?* A medida que vayamos iterando, nuestro vocabulario de tokens irá aumentando y nuestro length ira disminuyendo. El objetivo es encontrar un \"sweetspot\". Podemos elegir un tamaño de vocabulario y el número de iteraciones será...\n",
    "\n",
    "$$NIterations = DesiredSize - 256 $$\n",
    "\n",
    "Comenzamos con 256 (del 0 al 255 por los 8 bits de UTF8) y queremos añadir hasta llegar a nuestro tamaño de vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96bc415b-86db-41a5-a88b-27d667d54158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging (111, 32) into a new token 256\n",
      "Merging (101, 32) into a new token 257\n",
      "Merging (110, 32) into a new token 258\n",
      "Merging (97, 32) into a new token 259\n",
      "Merging (100, 257) into a new token 260\n",
      "Merging (44, 32) into a new token 261\n",
      "Merging (97, 114) into a new token 262\n",
      "Merging (260, 108) into a new token 263\n",
      "Merging (97, 110) into a new token 264\n",
      "Merging (111, 114) into a new token 265\n",
      "Merging (116, 105) into a new token 266\n",
      "Merging (117, 258) into a new token 267\n",
      "Merging (32, 263) into a new token 268\n",
      "Merging (99, 104) into a new token 269\n",
      "Merging (97, 261) into a new token 270\n",
      "Merging (110, 256) into a new token 271\n",
      "Merging (113, 117) into a new token 272\n",
      "Merging (101, 114) into a new token 273\n",
      "Merging (97, 99) into a new token 274\n",
      "Merging (100, 262) into a new token 275\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "ids = list(tokens)\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key = stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging {pair} into a new token {idx}\")\n",
    "\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c0fd7a-1665-4812-82d6-7a24726e3d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length:  179\n",
      "ids length:  113\n",
      "compression ratio: 1.58X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length: \", len(tokens))\n",
    "print(\"ids length: \", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a017b8-e724-41e3-a61a-b7ee5f8d753b",
   "metadata": {},
   "source": [
    "El tokenizador es algo completamente independiente al LLM. Tiene su propio set de entrenamiento de texto (que PODRÍA ser diferente al del LLM) que utilizas para entrenarlo utilizando el Byte Pair Encoding (BPE) Algorithm. Después de esto es capaz de codificar y decodificar una secuencia de texto a tokens y viceversa.\n",
    "\n",
    "Los datos para entrenar el tokenizador son importantes porque, por ejemplo, si tienes mucho texto en Japonés, significa que se formarán más tokens en Japonés.\n",
    "\n",
    "## **Decoding**\n",
    "\n",
    "No todos los bytes son decodificables, por ejejmplo el byte `128`. Entonces, si nuestro LLM no genera correctamente contenido puede ser que tengamos error en la decodificación. Para evitar esto, hacemos que, en vez de saltar un error, nos lo sustituya con `�`. Esto se consigue con `errors = \"replace\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77478d7-94ab-446f-8308-b95d94b27198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {idx : bytes([idx]) for idx in range(256)}\n",
    "\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors = \"replace\")\n",
    "    return text\n",
    "\n",
    "decode([128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8b67e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'o ',\n",
       " 257: b'e ',\n",
       " 258: b'n ',\n",
       " 259: b'a ',\n",
       " 260: b'de ',\n",
       " 261: b', ',\n",
       " 262: b'ar',\n",
       " 263: b'de l',\n",
       " 264: b'an',\n",
       " 265: b'or',\n",
       " 266: b'ti',\n",
       " 267: b'un ',\n",
       " 268: b' de l',\n",
       " 269: b'ch',\n",
       " 270: b'a, ',\n",
       " 271: b'no ',\n",
       " 272: b'qu',\n",
       " 273: b'er',\n",
       " 274: b'ac',\n",
       " 275: b'dar'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "236852ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(111, 32): 256,\n",
       " (101, 32): 257,\n",
       " (110, 32): 258,\n",
       " (97, 32): 259,\n",
       " (100, 257): 260,\n",
       " (44, 32): 261,\n",
       " (97, 114): 262,\n",
       " (260, 108): 263,\n",
       " (97, 110): 264,\n",
       " (111, 114): 265,\n",
       " (116, 105): 266,\n",
       " (117, 258): 267,\n",
       " (32, 263): 268,\n",
       " (99, 104): 269,\n",
       " (97, 261): 270,\n",
       " (110, 256): 271,\n",
       " (113, 117): 272,\n",
       " (101, 114): 273,\n",
       " (97, 99): 274,\n",
       " (100, 262): 275}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8521afd-61cd-4b2d-9de5-807f0fac1eb4",
   "metadata": {},
   "source": [
    "## **Encoding**\n",
    "Para hacer los mergeos es importante hacerlos en el mismo orden en el que se han hecho antes, ya que unos mergeos utilizan a otros generados anteriormente, por ejemplo:\n",
    "\n",
    "```\n",
    "aaabdaaabac\n",
    "```\n",
    "\n",
    "Se sustituyen las `aa` por `Z`:\n",
    "```\n",
    "ZabdZabac\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Se sustituyen las `ab` por `Y`:\n",
    "```\n",
    "ZYdZYac\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Se sustituyen las `ZY` por `X`:\n",
    "```\n",
    "XdXac\n",
    "X=ZY\n",
    "Y=ab\n",
    "Z=aa\n",
    "```\n",
    "\n",
    "Aquí no puedes utilizar el token `X` si no has generado antes el `Z` y el `Y`. Por esto hay que hacerlo en el mismo orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5a68fd-c5fd-446b-a75c-667082dc4a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[104, 101, 108, 108, 256, 119, 265, 108, 100, 33]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while len(tokens) > 2:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key = lambda p : merges.get(p, float(\"inf\"))) # Cogemos aquel que tiene el valor minimo de merges\n",
    "        if pair not in merges:\n",
    "            break # No se puede mergear nada\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "encode(\"hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e2a5c17-b64c-4cbb-881e-a6902fac080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760dcf08-2f6e-495f-b102-fb1bfdc043f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text = \"hello world!\"\n",
    "\n",
    "text2 = decode(encode(text))\n",
    "print(text == text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4dff64-1a1e-49f7-adbe-099ae497ef27",
   "metadata": {},
   "source": [
    "# **Complejizando el Tokenizador (GPT)**\n",
    "\n",
    "En OpenAI se dieron cuenta de que si codificas únicamente de esta manera puede llegar a tener problemas ya que puedes tener palabras muy comunes como \"perro\" codificadas de manera diferente, por ejemplo: `perro.`, `perro!`, `perro?`...etc. Esto puede generar conflicto ya que tienes una misma palabra pero con diferentes codificaciones por la puntuación. Por esto mismo, OpenAI en [GPT2](https://github.com/openai/gpt-2/blob/master/src/encoder.py) implementó unos patrones regex para que esto no succediera. Primero lo que hace es splittear el texto según los patrones de texto y luego lo codifica para evitar los problemas mencionados anteriormente. Es decir, que solo puedes encontrar merges dentro de los elementos splitteados y cuando ya no puedas mergear más los elementos se juntarán en una lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "466e8025-c115-45c7-8c0c-34a44637a25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', ' you', ' know', ' where', ' my', ' 1', 'st', ' dog', ' is', '?']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\", re.IGNORECASE)\n",
    "\n",
    "text = \"Do you know where my 1st dog is?\"\n",
    "print(re.findall(gpt2pat, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fa174-bb56-4aa0-9327-871a51a203a7",
   "metadata": {},
   "source": [
    "Sin emabrgo, hay más cosas además de esta, OpenAI no ha revelado el código de entrenamiento del tokenizador por lo cuál no podemos estar seguros de qué más normas han puesto. Sin embargo, hay cosas como los espacios que aunque el splitter los dividide y varios de estos podrían componer un token no lo hacen, esto hablando de GPT2, en GPT4 si que están unidos. La librería de OpenAI de los tokenizadores es `tiktoken` pero no se pueden entrenar tokenizadores, es una librería solo de inferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "840c40a3-8571-49e6-8553-2316c99088dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 18435, 2159]\n",
      "[262, 22691, 4435]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "## -- GPT2 (no une espacios)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    Hello World\"))\n",
    "\n",
    "## -- GPT4 (une espacios)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    Hello World\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7718013-6f85-4b35-b123-e9a7004a0f65",
   "metadata": {},
   "source": [
    "El patron de Regex también cambia para el [tokenizador de GPT4](https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py), se puede ver si bajas hasta `def cl100k_base()` y ves el pat_str. Pero no se sabe el por qué se hace así ya que no está documentado, tendrás sus razones pero no las podemos conocer.\n",
    "\n",
    "## **Conociendo el tokenizador de GPT2**\n",
    "Sabiendo todo esto, podemos entender el tokenizador ([`encoder.py`](https://github.com/openai/gpt-2/blob/master/src/encoder.py)) de GPT2, este depende de dos archivos (cómo se ve en el código): `encoder.json` y `vocab.bpe`. Estos archivos son para notros los objetos `merges` y `vocab` respectivamente. Lo vemos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8167bca-c88a-40b8-b7e0-79ffe2c507c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "with open(\"tiktoken/encoder.json\", \"r\") as f:\n",
    "    encoder = json.load(f)\n",
    "\n",
    "with open(\"tiktoken/vocab.bpe\", \"r\", encoding = \"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e4db2-f1c0-48d4-aa5e-437d44f5924c",
   "metadata": {},
   "source": [
    "Lo único confuso es que el tokenizador de GPT2 tiene un `byte_encoder` y un `byte_decoder`. Estos lo que hace es (Karpathy no lo entiende) es que primero hace una codificación a bytes y después un encode y después una decodificación a bytes antes de pasarlo a texto. El código es un poco diferente al nuestro pero algoritmicamente es igual a lo que se ha implementado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1325271-3c47-4d05-995a-c45242e1ea7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder) # 256 raw tokens + 50.000 merges + 1 token especial <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdc34e1d-ab24-40d2-aad5-fb23c461ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder[\"<|endoftext|>\"] # Sirve para delimitar textos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19043798-cd6a-4263-b814-ee2b49f6588e",
   "metadata": {},
   "source": [
    "Este último token no está en [`encoder.py`](https://github.com/openai/gpt-2/blob/master/src/encoder.py) pero sí en [`tiktoken`](https://github.com/openai/tiktoken/tree/main?tab=MIT-1-ov-file). Se añade después del entrenamiento, es decir, es independiente al entrenamiento y se añade manualmente y los que queramos. No sirve solo para poder delimitar textos sino también conversaciones y mensajes (ver [tiktokenizer](https://tiktokenizer.vercel.app/)).\n",
    "![Image](imgs/tiktokenizer.png)\n",
    "Se pueden añadir tokens especiales a los modelos, sin embargo, es importante mencionar que será necesario hacerle un poco de cirugía al modelo, ya que vas a tener que añadirle uno (si solo añades un token especial) al vocab size y hacer que sea capaz de predecir un token más. Se suele coger un modelo preentrenado y se inicializan solo las nuevas conexiones de manera random para que aprenda durante el fine tuning.\n",
    "\n",
    "# **Custom Tokenizer**\n",
    "En este momento ya podemos entrenar nuestro propio tokenizador.\n",
    "\n",
    "# **Sentencepiece**\n",
    "Nos alejamos de `tiktoken`. Esta librería es de Google y se utiliza mucho porque te permite hacer entrenamientos e inferencias de tokenizadores. Se utiliza en Llama y Mistral (seguirá siendo así?). [Ver Código](https://github.com/google/sentencepiece)\n",
    "\n",
    "La principal diferencia respecto a lo que hemos hecho es que cambian el orden de algunas cosas:\n",
    "* Tiktoken: Codificamos a bytes de UTF-8 y después unimos bytes con BPE.\n",
    "* Sentencepiece: No convierte a UTF-8 salvo que sea estrictamente necesario. Trabaja directamente con los code points de Unicode y recurre a UTF-8 solo en casos excepcionales (y es posible definir qué se considera \"excepcional\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29d8ad82-6216-4e62-b25a-e347d6528774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "with open(\"examples/toy_text.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e8ab02-1258-488e-b13f-4257e840f3ec",
   "metadata": {},
   "source": [
    "A continuación se ve una aproximación a los ajustes utilizados para entrenar Llama 2. Notas importantes:\n",
    "* Antes se solía normalizar mucho el texto, sin embargo, desde que aparecieron los LLM lo mejor es tocar lo menos posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c0d2cce-a0e0-4498-9ce6-28733e6b0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dict(\n",
    "    input = \"examples/toy_text.txt\",\n",
    "    input_format = \"text\",\n",
    "    model_prefix = \"tok400\", # El prefijo del filename\n",
    "    model_type = \"bpe\",\n",
    "    vocab_size = 400, # Obviamente llama tiene más\n",
    "    normalization_rule_name = \"identity\",\n",
    "    remove_extra_whitespaces = False,\n",
    "    input_sentence_size = 200_000_000,\n",
    "    max_sentence_length = 4192,\n",
    "    seed_sentencepiece_size = 1_000_000,\n",
    "    shuffle_input_sentence = True,\n",
    "    character_coverage = 0.99995,\n",
    "    byte_fallback = True, # Si esto es True, cuando no conozca algo pondrá el byte pero si es False pondra UNK \n",
    "    split_digits = True,\n",
    "    split_by_unicode_script = True,\n",
    "    split_by_whitespace = True,\n",
    "    split_by_number = True,\n",
    "    max_sentencepiece_length = 16,\n",
    "    add_dummy_prefix = True, # Esto hace que trate \"world\" con el mismo token que \"hello world\" añadiendo un \" \" delante.\n",
    "    allow_whitespace_only_pieces = True,\n",
    "    unk_id = 0, # Debe existir\n",
    "    bos_id = 1, # Beginning of Sentence\n",
    "    eos_id = 2, # End of Sentence\n",
    "    pad_id = -1, # Padding\n",
    "    num_threads = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b80b4b0-6bcf-489a-9aa8-5ce20efb469f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: examples/toy_text.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 4\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: examples/toy_text.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=504\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=39\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 58\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=18 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3 size=20 all=283 active=244 piece=ed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=40 all=305 active=266 piece=.]\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=60 all=324 active=285 piece=ken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2 size=80 all=334 active=295 piece=▁model\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1 size=100 all=338 active=299 piece=lo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1 min_freq=0\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e39a4003-2e9f-4bad-a3f6-7c2464a223f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tok400.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx ] for idx in range(sp.get_piece_size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ec99f7-19e2-4bdb-b5ea-021cafb46636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['en', 259],\n",
       " ['▁t', 260],\n",
       " ['ce', 261],\n",
       " ['in', 262],\n",
       " ['ra', 263],\n",
       " ['▁a', 264],\n",
       " ['de', 265],\n",
       " ['er', 266],\n",
       " ['▁s', 267],\n",
       " ['ent', 268],\n",
       " ['or', 269],\n",
       " ['pr', 270],\n",
       " ['▁m', 271],\n",
       " ['▁u', 272],\n",
       " ['ing', 273],\n",
       " ['▁th', 274],\n",
       " ['ence', 275],\n",
       " ['entence', 276],\n",
       " ['Pi', 277],\n",
       " ['ed', 278],\n",
       " ['em', 279],\n",
       " ['ex', 280],\n",
       " ['is', 281],\n",
       " ['iz', 282],\n",
       " ['la', 283],\n",
       " ['on', 284],\n",
       " ['st', 285],\n",
       " ['▁S', 286],\n",
       " ['Pie', 287],\n",
       " ['end', 288],\n",
       " ['ext', 289],\n",
       " ['▁an', 290],\n",
       " ['▁pr', 291],\n",
       " ['▁to', 292],\n",
       " ['▁un', 293],\n",
       " ['▁the', 294],\n",
       " ['Piece', 295],\n",
       " ['▁Sentence', 296],\n",
       " ['▁SentencePiece', 297],\n",
       " ['.]', 298],\n",
       " ['Ne', 299],\n",
       " ['ag', 300],\n",
       " ['do', 301],\n",
       " ['ec', 302],\n",
       " ['gu', 303],\n",
       " ['ic', 304],\n",
       " ['ir', 305],\n",
       " ['it', 306],\n",
       " ['ly', 307],\n",
       " ['to', 308],\n",
       " ['▁(', 309],\n",
       " ['▁[', 310],\n",
       " ['▁f', 311],\n",
       " ['▁n', 312],\n",
       " ['▁w', 313],\n",
       " ['.])', 314],\n",
       " ['age', 315],\n",
       " ['del', 316],\n",
       " ['ion', 317],\n",
       " ['ken', 318],\n",
       " ['lan', 319],\n",
       " ['ral', 320],\n",
       " ['wor', 321],\n",
       " ['yst', 322],\n",
       " ['▁Ne', 323],\n",
       " ['▁al', 324],\n",
       " ['▁de', 325],\n",
       " ['▁is', 326],\n",
       " ['▁ma', 327],\n",
       " ['▁mo', 328],\n",
       " ['izer', 329],\n",
       " ['rain', 330],\n",
       " ['ural', 331],\n",
       " ['▁and', 332],\n",
       " ['▁lan', 333],\n",
       " ['▁pre', 334],\n",
       " ['guage', 335],\n",
       " ['ystem', 336],\n",
       " ['▁text', 337],\n",
       " ['▁model', 338],\n",
       " ['▁train', 339],\n",
       " ['kenizer', 340],\n",
       " ['▁system', 341],\n",
       " ['▁language', 342],\n",
       " ['▁training', 343],\n",
       " ['.,', 344],\n",
       " ['BP', 345],\n",
       " ['Ku', 346],\n",
       " ['ab', 347],\n",
       " ['as', 348],\n",
       " ['at', 349],\n",
       " ['by', 350],\n",
       " ['co', 351],\n",
       " ['es', 352],\n",
       " ['et', 353],\n",
       " ['if', 354],\n",
       " ['ig', 355],\n",
       " ['im', 356],\n",
       " ['ke', 357],\n",
       " ['lo', 358],\n",
       " ['nr', 359],\n",
       " ['oc', 360],\n",
       " ['e', 361],\n",
       " ['▁', 362],\n",
       " ['n', 363],\n",
       " ['t', 364],\n",
       " ['i', 365],\n",
       " ['r', 366],\n",
       " ['a', 367],\n",
       " ['o', 368],\n",
       " ['s', 369],\n",
       " ['d', 370],\n",
       " ['c', 371],\n",
       " ['l', 372],\n",
       " ['u', 373],\n",
       " ['g', 374],\n",
       " ['m', 375],\n",
       " ['p', 376],\n",
       " ['.', 377],\n",
       " ['h', 378],\n",
       " ['-', 379],\n",
       " ['w', 380],\n",
       " ['y', 381],\n",
       " ['P', 382],\n",
       " ['S', 383],\n",
       " ['b', 384],\n",
       " ['f', 385],\n",
       " ['k', 386],\n",
       " [')', 387],\n",
       " ['x', 388],\n",
       " ['z', 389],\n",
       " ['(', 390],\n",
       " ['N', 391],\n",
       " ['[', 392],\n",
       " [']', 393],\n",
       " ['v', 394],\n",
       " [',', 395],\n",
       " ['/', 396],\n",
       " ['B', 397],\n",
       " ['E', 398],\n",
       " ['K', 399]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9516837a-2d41-424d-b3df-3fce6a2abeea",
   "metadata": {},
   "source": [
    "Sentencepiece representa el vocab size así:\n",
    "* Special tokens\n",
    "* Byte tokes\n",
    "* Merged tokens\n",
    "* Individual codepoints: Si estos son muy raros se pueden ignorar (character_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31d2f37f-317b-420f-a1d3-c2b6d300f4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[286, 198, 164, 384, 367, 301, 362, 381, 362, 301, 375, 273, 368]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = sp.encode(\"Sábado y domingo\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c40036ab-b720-45ce-be77-6c9a062f0f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁S', '<0xC3>', '<0xA1>', 'b', 'a', 'do', '▁', 'y', '▁', 'do', 'm', 'ing', 'o']\n"
     ]
    }
   ],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids]) # Vemos que no detecta la tílde porque no está en su set de entrenamiento, recurre a los bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdbe7d-708a-4276-8e97-456133574be7",
   "metadata": {},
   "source": [
    "# **Vocab Size**\n",
    "El vocab size únicamente aparece en los embeddings y en la capa linear al final (para predecir el siguiente token). Aumentar el vocab size tiene dos pegas principales:\n",
    "1. Aumentas el costo computacional\n",
    "2. Cada token está menos representado (puede darse el caso de que el modelo no aprenda por falta de representatividad)\n",
    "3. Más información está introducida en cada token y no se puede sacar toda la info que hay embedida\n",
    "\n",
    "Hoy en día está en las decenas de miles o, en algunos casos, en los cientos de miles. Si queremos añadir nuevos tokens, tenemos que cambiarle el tamaño a la capa de los embeddings y a la capa linear del final y congelar el resto del modelo.\n",
    "\n",
    "# **Multimodalidad**\n",
    "Lo que haces no es cambiar el modelo, sino cambiar el tokenizador, es decir, \"tokenizas\" la imagen con una CNN y le pasas las representaciones como si fuera texto, ya que el modelo no entiende de texto o imágenes, sino de tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
