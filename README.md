![BPETokenizer](imgs/BPETokenizer.png)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg) ![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)

# **BPETokenizer**
Una implementaci√≥n did√°ctica en Python del algoritmo Byte-Pair Encoding (BPE) (Sennrich et al., 2016).

Este mini proyecto es la implementaci√≥n del algoritmo Byte-Pair Encoding (BPE) para tokenizar texto como lo hacen los LLMs. Para una inmersi√≥n completa y pr√°ctica en el tema, se incluye el notebook interactivo [`explicaci√≥n_bpe.ipynb`](./explicaci√≥n_bpe.ipynb) [![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ErikSarriegui/BPETokenizer/blob/main//explicaci√≥n_bpe.ipynb), que sirve como gu√≠a paso a paso para entender cada componente del algoritmo. Es el punto de partida ideal si tu objetivo es aprender.

## **√çndice**
* Prop√≥sito y Advertencia
* Instalaci√≥n
* ¬øC√≥mo Usarlo?
    1. Preparar el Corpus
    2. Entrenar el Tokenizador
    3. Guardar y Cargar
    4. Tokenizar y De-tokenizar Texto
* ¬øC√≥mo Funciona el Algoritmo BPE?
* Licencia
* Agradecimientos
* Bibliograf√≠a


## ‚ö†Ô∏è**Prop√≥sito y Advertencia**‚ö†Ô∏è
Este proyecto es una implementaci√≥n sencilla del algoritmo BPE en Python que, pese a permitir el entrenamiento e inferencia de tokenizadores, no deber√≠a utilizarse en entornos de producci√≥n. Este proyecto tiene como objetivo comprender el algoritmo de Byte-Pair Encoding y el funcionamiento de los tokenizadores, si lo que buscas es utilizar tokenizadores de manera eficiente puedes ver los siguientes proyectos:
* [`tiktoken`](https://github.com/openai/tiktoken) de OpenAI.
* [`sentencepiece`](https://github.com/google/sentencepiece) de Google.
* [`tokenizers`](https://github.com/huggingface/tokenizers) de Hugging Face.

## **Instalaci√≥n**
El proyecto puede instalarse mediante `pip` con:
```bash
pip install git+https://github.com/ErikSarriegui/BPETokenizer 
```
Una vez instalado en el sistema ya est√° listo para utilizarse.

## **¬øC√≥mo usarlo?**
Para una gu√≠a de inicio r√°pido y un ejemplo pr√°ctico, puedes ejecutar el notebook `quickstart.ipynb` directamente en Google Colab:
[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ErikSarriegui/BPETokenizer/blob/main/quickstart.ipynb)

### 1. Preparar el Corpus
Primero necesitas preparar el texto que usar√°s para entrenar el tokenizador:

```python
texto_entrenamiento = "Tu texto de entrenamiento aqu√≠..."
```

### 2. Entrenar el Tokenizador
Para entrenar un nuevo tokenizador:

```python
from BPETokenizer import BPETrainer

# Crear el entrenador
trainer = BPETrainer()

# Entrenar el tokenizador
# vocab_size: n√∫mero total de tokens (incluyendo bytes base)
# verbose: muestra el progreso del entrenamiento
trainer.train(
    text = texto_entrenamiento,
    vocab_size = 1000,  # Por ejemplo, para 744 fusiones (1000-256)
    verbose = True
)
```

### 3. Guardar y Cargar
Para guardar el tokenizador entrenado:

```python
# Guardar el tokenizador
trainer.save("ruta/para/guardar")
```

Para cargar un tokenizador previamente entrenado:

```python
from BPETokenizer import BPETokenizer

# Cargar el tokenizador
tokenizer = BPETokenizer.from_dir("ruta/donde/guardaste")
```

### 4. Codificar y De-codificar Texto
Una vez cargado el tokenizador, puedes usarlo as√≠:

```python
# Tokenizar texto
texto = "Ejemplo de texto para tokenizar"
tokens = tokenizer.encode(texto)

# De-tokenizar
texto_reconstruido = tokenizer.decode(tokens)
```

El tokenizador utiliza por defecto el patr√≥n de regex de GPT-2 (Radford et al., 2019), pero puedes especificar un patr√≥n personalizado tanto en el entrenamiento como en la carga:

```python
# Usar patr√≥n personalizado
trainer = BPETrainer(pattern="tu_patron_regex")
# o
tokenizer = BPETokenizer.from_dir("ruta", pattern="tu_patron_regex")
```
## **¬øC√≥mo Funciona el Algoritmo BPE?**
Para entender c√≥mo funciona el tokenizador, es crucial descomponer el proceso en varias capas, desde c√≥mo se representa el texto en un ordenador hasta el algoritmo de fusi√≥n de pares de bytes. En el notebook `explicacion_bpe.ipynb` de este repositorio encontrar√°s una gu√≠a interactiva que demuestra cada uno de estos pasos con c√≥digo. Pero antes de nada, respondamos a una pregunta fundamental: Qu√© es un tokenizador? y ¬øPor qu√© necesitamos uno?

### 1. ¬øQu√© es un Tokenizador y por qu√© lo necesitamos?
Un modelo de lenguaje es, en esencia, una compleja m√°quina matem√°tica. No puede "leer" la palabra `hola` de la misma forma que nosotros, las m√°quinas solo pueden realizar operaciones matem√°ticas y no se puede operar con letras, es decir, no se puede, por ejemplo, multiplicar una `h` por una `o`. Por lo tanto, el primer paso en cualquier tarea de Procesamiento del Lenguaje Natural (NLP) es convertir el texto en una secuencia de n√∫meros que el modelo pueda procesar.

Este proceso de conversi√≥n se llama **tokenizaci√≥n**, y el encargado de hacerlo es el **tokenizador**.

**¬øPor qu√© no usar un m√©todo simple?**
Podr√≠amos pensar en soluciones sencillas, como asignar un n√∫mero √∫nico a cada palabra del diccionario. Sin embargo, esto presenta varios problemas:
*   **Vocabulario Gigante:** El n√∫mero de palabras en un idioma es enorme (¬°y no para de crecer!), lo que har√≠a que el modelo fuera inmanejable.
*   **Palabras Desconocidas (Out-of-Vocabulary):** ¬øQu√© hace el modelo cuando se encuentra una palabra que no vio durante el entrenamiento, como un neologismo, un error tipogr√°fico (`"ejemlpo"`) o un nombre propio raro?
*   **Relaciones entre Palabras:** Las palabras como "correr", "corriendo" o "corredor" comparten una ra√≠z com√∫n (`corr-`). Un sistema simple que asigna un ID diferente a cada una pierde esta valiosa informaci√≥n morfol√≥gica.

Aqu√≠ es donde brillan los algoritmos de tokenizaci√≥n por sub-palabras como **BPE**. En lugar de tratar las palabras como unidades indivisibles, BPE las descompone en "piezas" m√°s peque√±as y frecuentes. De esta manera, puede:
1.  **Manejar un vocabulario de tama√±o fijo y razonable.**
2.  **Representar cualquier palabra**, incluso las desconocidas, combinando las sub-palabras que s√≠ conoce. Por ejemplo, podr√≠a representar "tokenizando" como `["token", "izando"]`.
3.  **Capturar informaci√≥n morfol√≥gica**, ya que las ra√≠ces y sufijos comunes se convierten en tokens.

Ahora que entendemos *por qu√©* es necesario tokenizar, veamos *c√≥mo* el algoritmo BPE lo consigue, empezando por c√≥mo se representa el texto en un ordenador.

### 2. Strings en Python: Secuencias de Puntos de C√≥digo Unicode
Cuando trabajamos con texto en Python, como `"Hola üëã"`, no estamos manejando directamente los bytes. Un `str` de Python es una secuencia de **puntos de c√≥digo Unicode** (The Unicode Consortium). Unicode es un est√°ndar que asigna un n√∫mero √∫nico (un "punto de c√≥digo") a casi cualquier car√°cter, s√≠mbolo o emoji imaginable.

Podemos ver estos n√∫meros usando la funci√≥n `ord()`:
*   `ord('H')` devuelve `72`.
*   `ord('o')` devuelve `111`.
*   `ord('üëã')` devuelve `128075`.

La funci√≥n `chr()` hace lo contrario: `chr(128075)` nos devuelve `'üëã'`. Esto nos permite representar una inmensa variedad de lenguajes y s√≠mbolos de manera abstracta. Sin embargo, para almacenar o transmitir este texto, necesitamos una forma de codificar estos n√∫meros en bytes.

### 3. De Unicode a Bytes: La Codificaci√≥n UTF-8
Aqu√≠ es donde entra en juego la codificaci√≥n **UTF-8**. Es un formato de codificaci√≥n de longitud variable que traduce los puntos de c√≥digo Unicode a una secuencia de bytes. Su principal ventaja es que es muy eficiente:
*   Los caracteres del alfabeto ingl√©s b√°sico (ASCII) se codifican en **1 byte**.
*   Caracteres latinos con acentos (como '√°' o '√º') usan **2 bytes**.
*   Otros caracteres comunes (como el s√≠mbolo del Euro '‚Ç¨') usan **3 bytes**.
*   Los emojis y s√≠mbolos m√°s raros pueden usar hasta **4 bytes**.

En Python, podemos convertir un string a su representaci√≥n en bytes UTF-8 con `.encode("utf8")`:
```python
texto = "Hola üëã"
bytes_utf8 = texto.encode("utf8")
# >> b'Hola \xf0\x9f\x91\x8b'
```

Como vemos, "Hola " ocupa 5 bytes, y el emoji "üëã" se ha convertido en la secuencia de 4 bytes `\xf0\x9f\x91\x8b`.
La clave del BPE moderno es que opera directamente sobre esta secuencia de bytes UTF-8. Esto es fundamental, ya que garantiza que el tokenizador nunca se encontrar√° con un car√°cter "desconocido". Cualquier texto, en cualquier idioma, puede ser descompuesto en una secuencia de bytes. El vocabulario base del algoritmo son, por tanto, los 256 valores de byte posibles (de 0 a 255).

### 4. El Algoritmo Byte-Pair Encoding (BPE)
Una vez que tenemos nuestro corpus de texto convertido en secuencias de bytes, comienza el entrenamiento del BPE. El objetivo es encontrar una forma de representar el texto de manera m√°s compacta.

El algoritmo es un bucle iterativo muy simple:
*   **Inicializaci√≥n**: El vocabulario inicial contiene 256 tokens, uno por cada valor de byte posible (0-255).
*   **B√∫squeda**: Se analiza todo el texto de entrenamiento para encontrar el **par de tokens adyacentes m√°s frecuente**. Al principio, estos tokens ser√°n bytes individuales.
*   **Fusi√≥n (Merge)**: Se crea un **nuevo token** para representar a este par. El ID de este nuevo token ser√° el siguiente n√∫mero disponible (256, 257, 258, etc.). Esta regla de fusi√≥n (ej: `(101, 108) -> 256`) se almacena.
*   **Reemplazo**: Se vuelve a recorrer todo el texto y se reemplazan todas las ocurrencias del par m√°s frecuente por el nuevo token reci√©n creado.
*   **Repetir**: Se repiten los pasos un n√∫mero determinado de veces. Este n√∫mero lo definimos con `vocab_size`. Si queremos un vocabulario de 1000 tokens, realizaremos `1000 - 256 = 744` fusiones.

El resultado del entrenamiento es una **lista de fusiones ordenadas por prioridad**. Para tokenizar un texto nuevo, se aplican estas mismas fusiones en el mismo orden. Para de-tokenizar, simplemente se deshace el proceso, convirtiendo cada token en su secuencia de bytes correspondiente.

### 5. La Importancia de los Patrones Regex (Pre-tokenizaci√≥n)
Si aplicamos BPE directamente sobre un texto, el algoritmo podr√≠a fusionar bytes que cruzan los l√≠mites de las palabras (por ejemplo, la "e" final de "coche" y la "a" inicial de "azul" en "coche azul"). Esto es, por lo general, sem√°nticamente indeseable.

Para evitarlo, los tokenizadores modernos como el de GPT-2 realizan una **pre-tokenizaci√≥n** usando expresiones regulares (regex). Antes de aplicar BPE, el texto se divide en trozos m√°s peque√±os. El patr√≥n regex de GPT-2, por ejemplo, est√° dise√±ado para separar palabras, n√∫meros, puntuci√≥n y s√∫mbolos y contracciones comunes.

El algoritmo BPE se ejecuta de forma independiente **dentro de cada uno de estos trozos**. Esto asegura que las fusiones se mantengan dentro de los l√≠mites de las palabras y la puntuaci√≥n, generando tokens mucho m√°s intuitivos y √∫tiles para el modelo de lenguaje, como `" token"` (con un espacio al principio) o `"izar"`.

Puedes ver c√≥mo interact√∫an el regex y BPE en la web de [tiktokenizer](https://tiktokenizer.vercel.app/).

### 6. Tokens Especiales
Finalmente, a menudo necesitamos tokens que no se derivan del texto, sino que sirven como instrucciones para el modelo. Ejemplos comunes son:
*   `<|endoftext|>`: Indica el final de un documento.
*   `<|im_start|>` y `<|im_end|>`: Usados en modelos de chat para delimitar los mensajes.
*   `<|pad|>`: Un token de relleno para que todas las secuencias de un batch tengan la misma longitud.

Estos tokens se a√±aden al vocabulario **despu√©s del entrenamiento BPE**, asign√°ndoles IDs de token que no entren en conflicto con los existentes. Durante la tokenizaci√≥n, se tratan como palabras completas y no se descomponen.

## **Licencia**
Este proyecto se distribuye bajo los t√©rminos de la **Licencia MIT**.

Esto significa que tienes una gran libertad para hacer lo que quieras con el c√≥digo, siempre y cuando incluyas el aviso de derechos de autor y el texto de la licencia original en cualquier copia sustancial del software. En resumen, eres libre de:

*   **Usar** el software para cualquier prop√≥sito (comercial, personal, educativo, etc.).
*   **Modificar** el c√≥digo para adaptarlo a tus necesidades.
*   **Distribuir** copias del c√≥digo.
*   **Sublicenciar** el software, es decir, incorporarlo en tus propios proyectos, incluso si son de c√≥digo cerrado.

Para leer el texto legal completo, consulta el archivo `LICENSE` que se encuentra en la ra√≠z de este repositorio.

## **Agradecimientos**

Quiero ofrecer un agradecimiento especial y una atribuci√≥n directa a **Andrej Karpathy**.

Es fundamental ser completamente transparente: este proyecto no est√° simplemente *inspirado* por su trabajo; es, en gran medida, una **adaptaci√≥n y refactorizaci√≥n del c√≥digo** que √©l desarrolla en vivo en su extraordinario v√≠deo sobre tokenizadores (Karpathy, 2023). La l√≥gica central del algoritmo BPE y la estructura del entrenador (`BPETrainer`) siguen muy de cerca su implementaci√≥n.

El prop√≥sito sirve como un ejercicio personal para desgranar e interiorizar cada l√≠nea y concepto que Karpathy explica con una claridad magistral. Por esto mismo, si realmente quieres entender c√≥mo funciona un tokenizador desde los cimientos, ver el v√≠deo original de Karpathy no es solo una recomendaci√≥n, es la fuente primaria y la clase magistral de la que bebe todo este proyecto.

## **Bibliograf√≠a**
*   **Sennrich, R., Haddow, B., & Birch, A. (2016). *Neural Machine Translation of Rare Words with Subword Units*.** Art√≠culo seminal que propuso el uso de BPE en el contexto de la Traducci√≥n Autom√°tica Neuronal, sentando las bases para su uso en los LLMs modernos. [Enlace al paper](https://arxiv.org/abs/1508.07909).

*   **Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). *Language Models are Unsupervised Multitask Learners*.** El paper de GPT-2, cuya implementaci√≥n del tokenizador BPE, especialmente su patr√≥n de pre-tokenizaci√≥n mediante regex, es una referencia directa para este proyecto. [Enlace al paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).

*   **Karpathy, A. (2023). *Let's build the GPT Tokenizer*.** V√≠deo-tutorial y clase magistral que sirve como fuente primaria para la implementaci√≥n pr√°ctica y la estructura del c√≥digo de este repositorio. [Enlace al v√≠deo](https://www.youtube.com/watch?v=zduSFxRajkE).

*   **The Unicode Consortium. *The Unicode Standard*.** Est√°ndar que define los puntos de c√≥digo y la codificaci√≥n (como UTF-8) sobre los que se construye el procesamiento de texto moderno y la capa base de este tokenizador. [Sitio web oficial](https://home.unicode.org/).